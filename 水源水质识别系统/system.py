import pandas as pd
import os
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from pylab import *
from sklearn.metrics import accuracy_score
from mpl_toolkits.mplot3d import Axes3D  # ç©ºé—´ä¸‰ç»´ç”»å›¾
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import confusion_matrix

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']

# åŠ è½½æ•°æ®
df_water = pd.read_csv('data/data.csv')  # æœ¬åœ°åŠ è½½

# å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œæ¯”ä¾‹ä¸ºï¼š8:2
x, y = df_water.iloc[:, 1:].values, df_water.iloc[:, 0].values  # ä½¿ç”¨ilocå‡½æ•°ï¼Œç´¢å¼•æ•°æ®ã€‚æ˜¾ç„¶ï¼Œxä¸ºæ•°æ®é›†ï¼Œyä¸ºæ ‡ç­¾
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=0)

# æ•°æ®çš„æ ‡å‡†åŒ–
sc = StandardScaler()  # å¼•å…¥StandardScalerå‡½æ•°ï¼Œè¿›è¡Œæ ‡å‡†åŒ– x'=(x-ğ)/ğˆ
x_train_std = sc.fit_transform(x_train)  # å¯¹x_trainæ•°æ®é›†æ ‡å‡†åŒ–åè½¬åŒ–ï¼Œä¾¿äºè®¡ç®—ï¼Œä¸‹åŒ
x_test_std = sc.fit_transform(x_test)

# è®¡ç®—å‡å€¼å‘é‡
np.set_printoptions(precision=4)  # ä¿ç•™å››ä½å°æ•°
mean_vecs = []
for label in range(1, 6):  # æ ‡ç­¾æœ‰1,2,3,4,5å…±äº”ç±»
    mean_vecs.append(np.mean(x_train_std[y_train == label], axis=0))  # å°†æ±‚å¥½çš„å‡å€¼å‘é‡appendè¿›mean_vecsè¿™ä¸ªåˆ—è¡¨
# print("Mean Vectors %s:" % label,mean_vecs[label-1])

# è®¡ç®—ç±»å†…æ•£å¸ƒçŸ©é˜µ
k = 9  # æ•°æ®å…±æœ‰ä¹ä¸ªåˆ†é‡
Sw = np.zeros((k, k))  # å»ºç«‹ä¹è¡Œä¹åˆ—çš„æ–¹é˜µï¼Œå‡†å¤‡å­˜å‚¨ç±»å†…æ•£å¸ƒçŸ©é˜µ
for label, mv in zip(range(1, 6), mean_vecs):
    Si = np.zeros((k, k))
    Si = np.cov(x_train_std[y_train == label].T)  # åæ–¹å·®
    Sw += Si
# print("ç±»å†…æ•£å¸ƒçŸ©é˜µï¼š", Sw.shape[0], "*", Sw.shape[1])
# print("ç±»å†…æ•£å¸ƒçŸ©é˜µï¼š\n", Sw)
# print("ç±»å†…æ ‡ç­¾åˆ†å¸ƒï¼š",np.bincount(y_train)[1:])

# è®¡ç®—ç±»é—´æ•£å¸ƒçŸ©é˜µ
mean_all = np.mean(x_train_std, axis=0)  # æ±‚å¾—è®­ç»ƒé›†çš„å‡å€¼
Sb = np.zeros((k, k))  # å­˜æ”¾ç±»é—´æ•£å¸ƒçŸ©é˜µçš„ä¹è¡Œä¹åˆ—çš„æ–¹é˜µ
for i, col_mv in enumerate(mean_vecs):
    n = x_train[y_train == i + 1, :].shape[0]  # æ±‚å¾—æ¯ä¸€ç±»çš„æ ·æœ¬ä¸ªæ•°
    col_mv = col_mv.reshape(k, 1)  # åˆ—å‡å€¼å‘é‡
    mean_all = mean_all.reshape(k, 1)  # è½¬åŒ–ä¸ºåˆ—å‘é‡
    Sb += n * (col_mv - mean_all).dot((col_mv - mean_all).T)
# print("ç±»é—´æ•£å¸ƒçŸ©é˜µï¼š", Sb.shape[0], "*", Sb.shape[1])
# print("ç±»é—´æ•£å¸ƒçŸ©é˜µï¼š\n", Sb)

# è®¡ç®—å¹¿ä¹‰ç‰¹å¾å€¼
eigenvalues, eigenvectors = np.linalg.eig(np.linalg.inv(Sw).dot(Sb))  # ä½¿ç”¨np.linalg.eigå‡½æ•°æ±‚å‡ºæ•£å¸ƒçŸ©é˜µçš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ï¼Œå°†ç‰¹å¾å€¼å­˜å‚¨åˆ°eigenvaluesä¸­ï¼Œå°†ç‰¹å¾å‘é‡å­˜å‚¨åˆ°eigenvectorsä¸­ï¼Œinvä¸ºå–é€†ï¼Œdotä¸ºç‚¹ä¹˜
eigen_pairs = [(np.abs(eigenvalues[i]), eigenvectors[:, i]) for i in range(len(eigenvalues))]  # å¯¹ç‰¹å¾å€¼å–ç»å¯¹å€¼
eigen_pairs = sorted(eigen_pairs, key=lambda k: k[0], reverse=True)  # æŒ‰é™åºå¯¹ç‰¹å¾å€¼è¿›è¡Œæ’åº
# print(eigen_pairs[0][1][:, np.newaxis].real)  # ç¬¬ä¸€ç‰¹å¾å‘é‡
# print("ç‰¹å¾å€¼é™åºæ’åˆ—ï¼š")
# for eigenvalue in eigen_pairs:
#     print(eigenvalue[0])

# çº¿æ€§åˆ¤åˆ«æ•æ‰ï¼Œè®¡ç®—è¾¨è¯†åŠ›
tot = sum(eigenvalues.real)  # è®¡ç®—ä¹ä¸ªåˆ†é‡çš„æ€»ç‰¹å¾å€¼
discr = []
# discr=[(i/tot) for i in sorted(eigen_vals.real,reverse=True)]
for i in sorted(eigenvalues.real, reverse=True):
    discr.append(i / tot)
cum_discr = np.cumsum(discr)  # è®¡ç®—ç´¯åŠ æ–¹å·®
# name = ("R1", "G1", "B1", "R2", "G2", "B2", "R3", "G3", "B3")
# plt.bar(range(1, 10), discr, color=['r', 'g', 'b'], alpha=0.7, align='center', label='å„é˜¶é¢œè‰²çŸ©åˆ†é‡çš„è¾¨è¯†åŠ›', tick_label=name)
# plt.step(range(1, 10), cum_discr, where='mid', label='ç´¯åŠ è¾¨è¯†åŠ›', color='#000000')
# plt.ylabel('è¾¨è¯†åŠ›')
# plt.xlabel('å„é˜¶é¢œè‰²çŸ©åˆ†é‡å')
# plt.ylim([-0.1, 1.1])
# plt.legend(loc='best')
# plt.show()

# è½¬æ¢çŸ©é˜µ
w = np.hstack((eigen_pairs[0][1][:, np.newaxis].real, eigen_pairs[1][1][:, np.newaxis].real, eigen_pairs[2][1][:, np.newaxis].real))  # æ°´å¹³æ–¹å‘é“ºå¼€æ•°ç»„
# print(w)

# æ ·æœ¬æ•°æ®æŠ•å½±åˆ°ä½ç»´ç©ºé—´
x_train_lda = x_train_std.dot(w)
x_test_lda = x_test_std.dot(w)
# æ•°æ®æå–
# æå–æµ‹è¯•é›†å’Œè®­ç»ƒé›†è¿›è¿‡æŠ•å½±åçš„ä¸€é˜¶é¢œè‰²çŸ©
a1 = x_train_lda[:, 0]
a2 = x_train_lda[:, 1]
a3 = x_train_lda[:, 2]
b1 = x_test_lda[:, 0]
b2 = x_test_lda[:, 1]
b3 = x_test_lda[:, 2]
# å­˜å‚¨æ–‡ä»¶
np.savetxt('a1.txt', a1, fmt='%.3f', newline=os.linesep)
np.savetxt('a2.txt', a2, fmt='%.3f', newline=os.linesep)
np.savetxt('a3.txt', a3, fmt='%.3f', newline=os.linesep)
np.savetxt('b1.txt', b1, fmt='%.3f', newline=os.linesep)
np.savetxt('b2.txt', b2, fmt='%.3f', newline=os.linesep)
np.savetxt('b3.txt', b3, fmt='%.3f', newline=os.linesep)

# è¯»å–åˆ†ç±»åçš„æ•°æ®
# è®­ç»ƒé›†
# file1 = "data/train_1.txt"
# file2 = "data/train_2.txt"
# file3 = "data/train_3.txt"
# file4 = "data/train_4.txt"
# file5 = "data/train_5.txt"
# æµ‹è¯•é›†
file1 = "data/test_1.txt"
file2 = "data/test_2.txt"
file3 = "data/test_3.txt"
file4 = "data/test_4.txt"
file5 = "data/test_5.txt"
data1 = np.loadtxt(file1)
data2 = np.loadtxt(file2)
data3 = np.loadtxt(file3)
data4 = np.loadtxt(file4)
data5 = np.loadtxt(file5)

# æ•°æ®ï¼‘
x1 = data1[:, 1]
y1 = data1[:, 2]
z1 = data1[:, 3]

# æ•°æ®2

x2 = data2[:, 1]
y2 = data2[:, 2]
z2 = data2[:, 3]

# æ•°æ®3

x3 = data3[:, 1]
y3 = data3[:, 2]
z3 = data3[:, 3]


# æ•°æ®4

x4 = data4[:, 1]
y4 = data4[:, 2]
z4 = data4[:, 3]

# æ•°æ®5

x5 = data5[:, 1]
y5 = data5[:, 2]
z5 = data5[:, 3]

# ç»˜åˆ¶æ•£ç‚¹å›¾
fig = plt.figure()
ax = Axes3D(fig)
ax.scatter(x1, y1, z1, c='r', label='ç¬¬ä¸€ç±»ç‚¹')
ax.scatter(x2, y2, z2, c='g', label='ç¬¬äºŒç±»ç‚¹')
ax.scatter(x3, y3, z3, c='b', label='ç¬¬ä¸‰ç±»ç‚¹')
ax.scatter(x4, y4, z4, c='y', label='ç¬¬å››ç±»ç‚¹')
ax.scatter(x5, y5, z5, c='m', label='ç¬¬äº”ç±»ç‚¹')

# ç»˜åˆ¶å›¾ä¾‹
ax.legend(loc='best')

# æ·»åŠ åæ ‡è½´(é¡ºåºæ˜¯Z, Y, X)
ax.set_zlabel('R', fontdict={'size': 15, 'color': 'red'})
ax.set_ylabel('G', fontdict={'size': 15, 'color': 'red'})
ax.set_xlabel('B', fontdict={'size': 15, 'color': 'red'})

# å±•ç¤º
plt.title("LDAæ¨¡å‹-æµ‹è¯•é›†çš„åˆ†ç±»æ•£ç‚¹å›¾")
plt.show()

